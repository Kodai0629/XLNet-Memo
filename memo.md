# XLNet

## Overview

双方向コンテクストをモデル化する機能を持つBERTのようなデノイジング・オートエンコーディングに基づく事前学習は、自己回帰的言語モデルに基づく事前学習アプローチよりも優れた性能を達成する。しかし、マスクで入力を破損することに依存する BERT は、マスクされた位置間の依存性を無視しており、事前学習と調整の不一致に悩まされている。これらの長所と短所を考慮して、我々は一般化された自己回帰型の事前学習法であるXLNetを提案する。XLNetは、(1)因子化順序のすべての順列に対する期待される尤度を最大化することで、双方向の文脈を学習することを可能にし、(2)自己回帰型の定式化によりBERTの限界を克服する。さらに、XLNetは、最先端の自己回帰モデルであるTransformer-XLのアイデアを事前学習に統合しています。実証的には、同等の実験設定の下で、質問応答、自然言語推論、感情分析、文書ランキングなどの20のタスクにおいて、XLNetはBERTを上回り、多くの場合、大差をつけています。

##### ヒント

* 特定の注目パターンは、`perm_mask` を使用して、トレーニング時およびテスト時に制御することができます。

* 様々な因数分解の順序で完全な自動回帰モデルを学習するのは難しいため、XLNetは`target_mapping`で選択された出力トークンのサブセットのみをターゲットとして事前学習されます。

* XLNetをシーケンシャルデコーディングに使用する場合（完全な双方向設定ではない）、`perm_mask`と`target_mapping`を使用して、注目度と出力を制御します（examples/pytorch/text-generation/run_generation.pyの例を参照）。

* XLNetは、配列長の制限がない数少ないモデルの一つです。

このモデルはthomwolf氏によって提供されました。オリジナルのコードは[こちら](https://github.com/zihangdai/xlnet/)にあります。

## XLNetConfig  
(使うことはなさそう)

[オリジナル](https://huggingface.co/transformers/model_doc/xlnet.html#xlnetconfig)

XLNetModelまたはTFXLNetModelのコンフィギュレーションを格納するコンフィギュレーションクラスです。これは、指定された引数に従ってXLNetモデルをインスタンス化するために使用され、モデルのアーキテクチャを定義します。デフォルトでコンフィギュレーションをインスタンス化すると、xlnet-large-casedアーキテクチャと同様のコンフィギュレーションが得られます。

ConfigurationオブジェクトはPretrainedConfigを継承し、モデルの出力を制御するために使用することができます。

## XLNetTokenizer

XLNetのトークナイザーを構築します。SentencePieceをベースにしています。
>SentencePieceとは
>
>日本語のようなスペースで単語が区切られていない言語にも対応したトークナイザーである。

##### パラメータ
* **vocab_file** (str)  トークンナイザをインスタンス化するために必要な語彙を含むSentencePieceファイル（通常、.spmという拡張子がついています）。

* **do_lower_case** (bool, オプション, デフォルトは `True`)  トークン化の際に入力を小文字にするかどうか。

* **remove_space** (bool, オプション, デフォルトは `True`)  トークン化の際にテキストをストリップするかどうか（文字列の前後にある余分なスペースを削除する）。

* **keep_accents**(bool, オプション, デフォルトは `False`)  トークン化の際にアクセント記号を保持するかどうか。

bos_token (str, オプション, デフォルトは `"<s>"` )  トークン化の際に、アクセントを維持するかどうか。

事前学習の際に使用された，シーケンスの始まりのトークンです。シーケンス分類器のトークンとして使用できます。

##### 注意
特殊なトークンを使用してシーケンスを構築する場合、これはシーケンスの先頭に使用されるトークンではありません。使用されるトークンはcls_tokenです。

* **eos_token** (str, オプション, デフォルトは`"</s>"`)  シーケンスの終わりのトークン。

##### 注意
特殊なトークンを使用してシーケンスを構築する場合、これはシーケンスの終わりに使用されるトークンではありません。使用されるトークンはsep_tokenです。

* **unk_token** (str, オプション, デフォルトは `"<unk>"`)  未知のトークンです。ボキャブラリーにないトークンはIDに変換できず、代わりにこのトークンが設定されます。

* **sep_token** (str, オプション, デフォルトは `"<sep>"`)  セパレータ・トークンです。これは、複数のシーケンスからシーケンスを構築する際に使用されます。例えば、シーケンス分類のための2つのシーケンスや、質問応答のためのテキストと質問などです。また、特別なトークンで構築されたシーケンスの最後のトークンとしても使用されます。

* **pad_token** (str, オプション, デフォルトは `"<pad>"`)  異なる長さのシーケンスをバッチ処理するときなど、パディングに使われるトークンです。

* **cls_token** (str, オプション, デフォルトは `"<cls>"`) シーケンスの分類（トークンごとの分類ではなく、シーケンス全体の分類）を行う際に使用される分類器のトークンです。特別なトークンで構築された場合は、シーケンスの最初のトークンです。

* **mask_token** (str, オプション, デフォルトは `"<mask>"`)  値のマスキングに使われるトークンです。これは、このモデルをマスキングされた言語モデリングでトレーニングするときに使われるトークンです。これは、このモデルが予測しようとするトークンです。

* **additional_special_tokens** (`List[str]`, オプション, デフォルトは `["<eop>", "<eod>"]`)  トークナイザーが使用する追加の特殊トークン。

* **sp_model_kwargs** (dict, オプション) - 文章を入力する際に使用されます。

  SentencePieceProcessor.__init__()メソッドに渡されます。SentencePieceのPythonラッパーは、特に以下の設定に使用できます。

  * enable_sampling: サブワードの正則化を有効にします。

  * nbest_size: unigramのサンプリングパラメータ。BPE-Dropoutでは無効。

    * nbest_size = {0,1}: サンプリングを行いません。

    * nbest_size > 1: nbest_sizeの結果からサンプリングを行う。

    * nbest_size < 0: nbest_sizeが無限であると仮定して、順方向フィルタリング・逆方向サンプリングアルゴリズムを用いて、すべての仮説（格子）からサンプリングする。

   * alpha: unigramサンプリングの平滑化パラメータ、BPE-dropoutのためのマージ操作のドロップアウト確率。

***sp_model***

すべての変換（文字列、トークン、ID）に使用されるSentencePieceプロセッサ。

  >***タイプ***
  >
  >`SentencePieceProcessor`
  
  
  
